from flask import Flask, request, jsonify, render_template, make_response, Response, stream_with_context
from werkzeug.exceptions import HTTPException
import os
import logging
from logging.handlers import RotatingFileHandler
import threading
import uuid
import io
import time
import hmac
import hashlib
import json as _json
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from flask_cors import CORS
from typing import List, Dict, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
from utils.text import normalize_text as _normalize_text
from utils.text import tokenize_fa as _tokenize_fa
from utils.text import split_paragraphs as _split_paragraphs
from utils.text import extract_text_from_html
from services.learner import VectorLearner

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": os.getenv('CORS_ORIGINS', '*')}})
limiter = Limiter(get_remote_address, app=app, default_limits=[os.getenv('RATE_LIMIT_DEFAULT', '60 per minute')])

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CONFIG_PATH = os.path.join(BASE_DIR, 'config.json')

# Ù¾ÙˆØ´Ù‡Ù” Ù¾ÛŒØ´â€ŒÙØ±Ø¶ PDFÙ‡Ø§ (Ø®ÙˆØ§Ù†Ø¯Ù† Ø®ÙˆØ¯Ú©Ø§Ø±)
DEFAULT_PDF_DIR = os.path.join(BASE_DIR, 'pdf')
VECTOR_INDEX_DIR = os.path.join(BASE_DIR, 'vector_index')
SOURCES_DIR = os.path.join(BASE_DIR, 'sources')

# ØªÙ†Ø¸ÛŒÙ… Ø³Ø§Ø¯Ù‡Ù” Ù„Ø§Ú¯â€ŒÙ‡Ø§
def _configure_logging():
    level_name = os.getenv('LOG_LEVEL', 'INFO').upper()
    level = getattr(logging, level_name, logging.INFO)
    logging.basicConfig(level=level, format='[%(asctime)s] %(levelname)s %(message)s')

_configure_logging()
logger = logging.getLogger("app")

# Ø³Ø§Ø®Øª Ù„Ø§Ú¯ JSON Ø¨Ø§ Ú†Ø±Ø®Ø´ ÙØ§ÛŒÙ„
def _setup_json_logging():
    try:
        logs_dir = os.path.join(BASE_DIR, 'logs')
        os.makedirs(logs_dir, exist_ok=True)
        file_path = os.path.join(logs_dir, 'app.log')

        class JsonFormatter(logging.Formatter):
            def format(self, record: logging.LogRecord) -> str:
                payload = {
                    'ts': int(time.time()),
                    'level': record.levelname,
                    'msg': record.getMessage(),
                    'name': record.name,
                }
                if hasattr(record, 'extra') and isinstance(record.extra, dict):
                    payload.update(record.extra)
                return _json.dumps(payload, ensure_ascii=False)

        handler = RotatingFileHandler(file_path, maxBytes=2 * 1024 * 1024, backupCount=5, encoding='utf-8')
        handler.setFormatter(JsonFormatter())
        root = logging.getLogger()
        root.addHandler(handler)
    except Exception:
        pass

_setup_json_logging()

# Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ†Ú¯Ø§Ø± Ø³Ø§Ø¯Ù‡: Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¯Ø± Ù„Ø§Ú¯ Ø«Ø¨Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯
@app.before_request
def _before_request_timer():
    try:
        setattr(request, '_start_time', time.perf_counter())
    except Exception:
        pass

@app.after_request
def _after_request_timer(response):
    try:
        start = getattr(request, '_start_time', None)
        if start is not None:
            dur_ms = int((time.perf_counter() - start) * 1000)
            logger.info("%s %s %s %dms", request.method, request.path, str(response.status_code), dur_ms)
    except Exception:
        pass
    return response

# Ø¢Ù¾Ù„ÙˆØ¯ Ø­Ø°Ù Ø´Ø¯Ù‡ Ø§Ø³ØªØ› Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø§Ù„Ú¯ÙˆÛŒ Ù¾Ø³ÙˆÙ†Ø¯Ù‡Ø§ Ù†ÛŒØ³Øª

# ----------- Ø°Ø®ÛŒØ±Ù‡Ù” Ù…ØªÙ†â€ŒÙ‡Ø§ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ù†Ø§Ø³Ù‡Ù” Ú©Ø§Ø±Ø¨Ø± (Ú©ÙˆÚ©ÛŒ) -----------
SESSION_CONTEXTS: Dict[str, List[str]] = {}
_context_lock = threading.Lock()
SESSION_PROCESSED: Dict[str, Set[str]] = {}
TOTAL_TEXTS_COUNT: int = 0
PROCESSED_FILES_COUNT: int = 0
PREWARM_PROGRESS: Dict[str, Dict[str, object]] = {}

# Ù†Ù…Ø§ÛŒÙ‡Ù” Ø³Ø¨Ú© Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø³Ø±ÛŒØ¹ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ (Ø¨Ø¯ÙˆÙ† ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø®Ø§Ø±Ø¬ÛŒ)
# per-session index: client_id -> { 'paragraphs': List[str], 'inverted': Dict[str, Set[int]] }
SESSION_INDEX: Dict[str, Dict[str, object]] = {}

# Ù†Ù…Ø§ÛŒÙ‡Ù”å‘å‘å‘å‘å‘ (FAISS) Ø³Ø±Ø§Ø³Ø±ÛŒ: Ø¨Ø±Ø§ÛŒ Â«ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒÂ» Ù‚ÙˆØ§Ù†ÛŒÙ† Ùˆ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø±Ø¯Ø§Ø±ÛŒ
GLOBAL_VECTOR = {
    'index': None,           # faiss.Index ÛŒØ§ None
    'paragraphs': [],        # List[str]
    'model_name': None,      # Ù†Ø§Ù… Ù…Ø¯Ù„ ØªØ¹åµŒÙ‡
}
EMBEDDER = { 'model': None, 'name': None }
LEARNER = VectorLearner(BASE_DIR)

# Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡Ù” Ø§Ù†Ø³Ø§Ù†ÛŒ (Human-in-the-loop)
CURATED_ANSWERS: Dict[str, Dict[str, object]] = {}

# ÙˆØ¶Ø¹ÛŒØª Ø¢Ø®Ø±ÛŒÙ† ØªÙ…Ø§Ø³ Ù…Ø¯Ù„
MODEL_STATUS: Dict[str, object] = {
    'engine': 'local',
    'error': '',
    'latency_ms': 0,
    'ts': 0,
}

def _read_config_file() -> Dict[str, object]:
    try:
        import json as _json
        if os.path.exists(CONFIG_PATH):
            with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
                return _json.load(f) or {}
    except Exception:
        pass
    return {}

def _write_config_file(cfg: Dict[str, object]) -> bool:
    try:
        import json as _json
        with open(CONFIG_PATH, 'w', encoding='utf-8') as f:
            _json.dump(cfg or {}, f, ensure_ascii=False, indent=2)
        return True
    except Exception:
        return False

def _mask_secret(value: str) -> str:
    if not value:
        return ''
    if len(value) <= 6:
        return '*' * len(value)
    return ('*' * (len(value) - 4)) + value[-4:]

def _load_persisted_config_into_env() -> None:
    cfg = _read_config_file()
    # DeepSeek
    dk = str(cfg.get('DEEPSEEK_API_KEY') or '').strip()
    dm = str(cfg.get('DEEPSEEK_MODEL') or '').strip()
    if dk:
        os.environ['DEEPSEEK_API_KEY'] = dk
        os.environ['USE_DEEPSEEK'] = '1'
    if dm:
        os.environ['DEEPSEEK_MODEL'] = dm
    # Optional toggles
    if str(cfg.get('USE_OLLAMA') or ''):
        os.environ['USE_OLLAMA'] = str(cfg.get('USE_OLLAMA'))
    if str(cfg.get('USE_LLAMA') or ''):
        os.environ['USE_LLAMA'] = str(cfg.get('USE_LLAMA'))

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡ Ù‡Ù†Ú¯Ø§Ù… Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ ÙØ±Ø¢ÛŒÙ†Ø¯
_load_persisted_config_into_env()

def _get_or_create_client_id() -> str:
    client_id = request.cookies.get('client_id')
    if client_id and isinstance(client_id, str) and len(client_id) >= 8:
        return client_id
    return uuid.uuid4().hex

def _is_fast_mode() -> bool:
    # Ø­Ø§Ù„Øª Ø³Ø±ÛŒØ¹ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ùˆ Ù…Ø­Ø¯ÙˆØ¯Ø³Ø§Ø²ÛŒ Ø²Ù…ÛŒÙ†Ù‡
    return _str_to_bool(os.getenv('FAST_MODE', '1'), default=True)

# Ù…ØªÙ†: Ø¨Ù‡ utils/text Ù…Ù†ØªÙ‚Ù„ Ø´Ø¯

def _ensure_dirs():
    try:
        os.makedirs(VECTOR_INDEX_DIR, exist_ok=True)
    except Exception:
        pass
    try:
        os.makedirs(SOURCES_DIR, exist_ok=True)
    except Exception:
        pass

def _get_embedder():
    # cached embedder
    if EMBEDDER.get('model') is not None:
        return EMBEDDER['model'], EMBEDDER.get('name')
    try:
        from sentence_transformers import SentenceTransformer
        model_name = os.getenv('EMBED_MODEL', 'paraphrase-multilingual-MiniLM-L12-v2')
        model = SentenceTransformer(model_name)
        EMBEDDER['model'] = model
        EMBEDDER['name'] = model_name
        return model, model_name
    except Exception:
        return None, None

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† HTML: Ø¨Ù‡ utils/text Ù…Ù†ØªÙ‚Ù„ Ø´Ø¯

def _faiss_available():
    try:
        import faiss  # type: ignore
        _ = faiss
        return True
    except Exception:
        return False

def build_global_vector_index(sources: List[str] | None = None) -> Dict[str, object]:
    """Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø¨Ø§ VectorLearner Ø§Ø² Ø³Ø´Ù†â€ŒÙ‡Ø§ Ùˆ Ù¾ÙˆØ´Ù‡Ù” Ù¾ÛŒØ´â€ŒÙØ±Ø¶."""
    _ensure_dirs()
    texts: List[str] = []
    with _context_lock:
        for arr in (SESSION_CONTEXTS.values() or []):
            for t in (arr or []):
                texts.append(t)
    srcs = sources or [DEFAULT_PDF_DIR]
    for src in srcs:
        tlist, _previews, _cnt = ingest_pdfs_from_dir(src, recursive=True, client_id=None, fast=True)
        texts.extend(tlist)
    res = LEARNER.build_from_texts(texts)
    # Ù‡Ù…Ú¯Ø§Ù…â€ŒØ³Ø§Ø²ÛŒ ÙˆØ¶Ø¹ÛŒØª Ù‚Ø¯ÛŒÙ…ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ù¾Ù†Ù„
    if res.get('built'):
        GLOBAL_VECTOR['index'] = LEARNER.index
        GLOBAL_VECTOR['paragraphs'] = LEARNER.paragraphs
        GLOBAL_VECTOR['model_name'] = LEARNER.model_name
    return res

def _load_global_vector_index_if_exists():
    if GLOBAL_VECTOR.get('index') is not None:
        return True
    if LEARNER.load_if_exists():
        GLOBAL_VECTOR['index'] = LEARNER.index
        GLOBAL_VECTOR['paragraphs'] = LEARNER.paragraphs
        GLOBAL_VECTOR['model_name'] = LEARNER.model_name
        return True
    return False

def _retrieve_with_vector(query: str, top_k: int = 5) -> List[Dict[str, object]]:
    if not query:
        return []
    if LEARNER.index is None and not _load_global_vector_index_if_exists():
        return []
    return LEARNER.search(query, top_k=top_k)

def _ensure_session_index(client_id: str) -> None:
    if not client_id:
        return
    with _context_lock:
        idx = SESSION_INDEX.get(client_id)
        texts = SESSION_CONTEXTS.get(client_id) or []
    if idx and idx.get('built_from_count') == len(texts):
        return
    paragraphs: List[str] = []
    for t in texts:
        paragraphs.extend(_split_paragraphs(t))
    inverted: Dict[str, Set[int]] = {}
    for i, p in enumerate(paragraphs):
        for tok in set(_tokenize_fa(p)):
            s = inverted.get(tok)
            if s is None:
                s = set()
                inverted[tok] = s
            s.add(i)
    with _context_lock:
        SESSION_INDEX[client_id] = {
            'paragraphs': paragraphs,
            'inverted': inverted,
            'built_from_count': len(texts),
        }

def _retrieve_paragraphs(question: str, client_id: str, top_k: int = 5) -> List[Dict[str, object]]:
    # Ø§Ø¨ØªØ¯Ø§ ØªÙ„Ø§Ø´ Ø¨Ø§ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯)
    vec = _retrieve_with_vector(question, top_k=top_k)
    if vec:
        return vec
    # Ø³Ù¾Ø³ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…Ø­Ù„ÛŒ Ø³Ø´Ù†
    if not client_id:
        return []
    with _context_lock:
        idx = SESSION_INDEX.get(client_id)
    if not idx:
        return []
    paragraphs: List[str] = idx.get('paragraphs') or []
    inverted: Dict[str, Set[int]] = idx.get('inverted') or {}
    q_tokens = _tokenize_fa(question)
    if not q_tokens:
        return []
    from collections import Counter
    scores = Counter()
    for qt in set(q_tokens):
        for pid in inverted.get(qt, set()):
            scores[pid] += 1
    if not scores:
        return []
    ranked = scores.most_common(top_k)
    results: List[Dict[str, object]] = []
    for pid, sc in ranked:
        snippet = paragraphs[pid]
        results.append({'pid': pid, 'score': int(sc), 'snippet': snippet})
    return results

def _build_limited_context(texts: List[str], per_text_limit: int = 1200, total_limit: int = 9000) -> str:
    # Ø¨Ø±Ø´ Ù‡Ø± Ù…ØªÙ† Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø­Ø¯Ø§Ú©Ø«Ø± Ú©Ù„ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§
    if not texts:
        return ''
    clipped_parts: List[str] = []
    remaining = max(1000, int(total_limit))
    per_limit = max(300, int(per_text_limit))
    for t in texts:
        if remaining <= 0:
            break
        part = (t or '')[:per_limit]
        if not part:
            continue
        if len(part) > remaining:
            part = part[:remaining]
        clipped_parts.append(part)
        remaining -= len(part)
    return "\n\n---\n\n".join(clipped_parts)

def _append_session_context(client_id: str, texts: List[str]) -> None:
    added = [t for t in texts if t]
    if not added:
        return
    global TOTAL_TEXTS_COUNT
    with _context_lock:
        existing = SESSION_CONTEXTS.get(client_id) or []
        existing.extend(added)
        SESSION_CONTEXTS[client_id] = existing
        TOTAL_TEXTS_COUNT += len(added)

# ----------- Ú©Ù…Ú©ÛŒ: Ø®ÙˆØ§Ù†Ø¯Ù† PDFÙ‡Ø§ Ø§Ø² ÛŒÚ© Ù¾ÙˆØ´Ù‡ -----------
def ingest_pdfs_from_dir(dir_path: str, recursive: bool = True, max_files: int = 200, client_id: str | None = None, fast: bool = False):
    dir_path = os.path.expanduser(dir_path)
    if not os.path.isabs(dir_path):
        dir_path = os.path.abspath(os.path.join(BASE_DIR, dir_path))
    if not os.path.isdir(dir_path):
        return [], [], 0

    def iter_pdfs(path: str):
        if recursive:
            for root, _dirs, files in os.walk(path):
                for name in files:
                    if name.lower().endswith('.pdf'):
                        yield os.path.join(root, name)
        else:
            for name in os.listdir(path):
                fp = os.path.join(path, name)
                if os.path.isfile(fp) and name.lower().endswith('.pdf'):
                    yield fp

    saved_texts: List[str] = []
    previews: List[str] = []
    total_files = 0
    errors_count = 0
    start_ts = time.time()
    try:
        max_seconds = float(os.getenv('INGEST_MAX_SECONDS', '30'))
    except Exception:
        max_seconds = 30.0
    # Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù‚Ø¨Ù„Ø§Ù‹ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø³Ø´Ù†
    processed: Set[str] = set()
    if client_id:
        with _context_lock:
            processed = set(SESSION_PROCESSED.get(client_id) or set())
    new_processed: Set[str] = set()
    logger.info("Ingest start: %s (recursive=%s)", dir_path, recursive)
    # Ú¯Ø±Ø¯Ø¢ÙˆØ±ÛŒ ÛŒÚ© Ø¨Ú† ÙØ§ÛŒÙ„ ØªØ§ Ø³Ù‚Ù max_files
    batch_paths: List[str] = []
    for fp in iter_pdfs(dir_path):
        abs_fp = os.path.abspath(fp)
        if abs_fp in processed:
            continue
        batch_paths.append(abs_fp)
        if len(batch_paths) >= max_files:
            break

    def _extract(path_abs: str) -> tuple[str, str, str]:
        try:
            text = extract_text_fast(path_abs) if fast else extract_text(path_abs)
        except Exception:
            logger.exception("Extract failed: %s", path_abs)
            nonlocal errors_count
            errors_count += 1
            text = ''
        if text:
            pv = (text[:500] + '...') if len(text) > 500 else text
            return (path_abs, text, pv)
        return (path_abs, '', '')

    if batch_paths:
        # Ø§Ø¬Ø±Ø§ÛŒ Ù…ÙˆØ§Ø²ÛŒ
        try:
            workers = int(os.getenv('INGEST_WORKERS', str(max(2, min(8, (os.cpu_count() or 4))))))
        except Exception:
            workers = max(2, min(8, (os.cpu_count() or 4)))
        try:
            with ThreadPoolExecutor(max_workers=workers) as pool:
                futures = [pool.submit(_extract, p) for p in batch_paths]
                for fut in as_completed(futures):
                    path_abs, text, pv = fut.result()
                    if text:
                        saved_texts.append(text)
                        previews.append(pv)
                        new_processed.add(path_abs)
                        total_files += 1
                    if (time.time() - start_ts) > max_seconds:
                        logger.warning("Ingest time budget exceeded (%ss)", max_seconds)
                        break
        except Exception:
            # fallback Ø³Ø±ÛŒØ§Ù„ÛŒ
            for p in batch_paths:
                path_abs, text, pv = _extract(p)
                if text:
                    saved_texts.append(text)
                    previews.append(pv)
                    new_processed.add(path_abs)
                    total_files += 1
                if (time.time() - start_ts) > max_seconds:
                    logger.warning("Ingest time budget exceeded (serial) (%ss)", max_seconds)
                    break

    if client_id and new_processed:
        with _context_lock:
            global PROCESSED_FILES_COUNT
            already = SESSION_PROCESSED.get(client_id) or set()
            already.update(new_processed)
            SESSION_PROCESSED[client_id] = already
            PROCESSED_FILES_COUNT += len(new_processed)
    logger.info("Ingest done: files=%d errors=%d elapsed_ms=%d", total_files, errors_count, int((time.time()-start_ts)*1000))
    return saved_texts, previews, total_files

# ----------- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² PDF -----------
def extract_text_from_pdf(pdf_path):
    # import ØªÙ†Ø¨Ù„ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø§Ú¯Ø± PyMuPDF Ù†ØµØ¨ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø­Ø¯Ø§Ù‚Ù„ Ø¨Ø§Ù„Ø§ Ø¨ÛŒØ§ÛŒØ¯
    try:
        import fitz  # PyMuPDF
    except Exception as exc:
        raise RuntimeError("Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ PyMuPDF (Ø¨Ø³ØªÙ‡ pymupdf) Ù†ØµØ¨ Ù†ÛŒØ³Øª. Ø¯Ø³ØªÙˆØ± Ù†ØµØ¨: pip install pymupdf") from exc

    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text("text")
    return text.strip()

# ØªÙ„Ø§Ø´ Ø¯ÙˆÙ…: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§ pdfminer.six (Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø®ÛŒ PDFÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ú©Ù‡ PyMuPDF Ù…ØªÙ† Ù†Ù…ÛŒâ€ŒØ¯Ù‡Ø¯)
def extract_text_with_pdfminer(pdf_path):
    try:
        from pdfminer.high_level import extract_text as pdfminer_extract_text
    except Exception:
        return ""
    try:
        mined = pdfminer_extract_text(pdf_path)
        return (mined or "").strip()
    except Exception:
        return ""

def extract_text_via_ocr(pdf_path):
    # OCR Ø¨Ø±Ø§ÛŒ PDFÙ‡Ø§ÛŒ Ø§Ø³Ú©Ù†â€ŒØ´Ø¯Ù‡ (Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ Ù…ØªÙ†)
    try:
        import fitz  # Ø¨Ø±Ø§ÛŒ Ø±Ù†Ø¯Ø± ØµÙØ­Ø§Øª PDF Ø¨Ù‡ ØªØµÙˆÛŒØ±
        from PIL import Image  # pillow
        import pytesseract
    except Exception as exc:
        raise RuntimeError("Ø¨Ø±Ø§ÛŒ OCR Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù†ØµØ¨ pillow Ùˆ pytesseract Ùˆ Tesseract OCR Ø¯Ø§Ø±ÛŒØ¯.") from exc

    # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ ØªØ¹ÛŒÛŒÙ† Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø³ÛŒØ± tesseract.exe Ø¯Ø± ÙˆÛŒÙ†Ø¯ÙˆØ²
    if os.name == 'nt':
        # Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ Ù…Ø´Ø®Øµ Ú©Ø±Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ØŒ Ù…Ù‚Ø¯Ù… Ø§Ø³Øª
        user_path = os.getenv('TESSERACT_PATH')
        candidates = []
        if user_path:
            candidates.append(user_path)
        # Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù…Ø¹Ù…ÙˆÙ„ Ù†ØµØ¨
        candidates.extend([
            r"C:\\Program Files\\Tesseract-OCR\\tesseract.exe",
            r"C:\\Program Files (x86)\\Tesseract-OCR\\tesseract.exe",
        ])
        for p in candidates:
            if os.path.exists(p):
                try:
                    pytesseract.pytesseract.tesseract_cmd = p
                    break
                except Exception:
                    pass

    doc = fitz.open(pdf_path)
    ocr_text_parts = []
    # Ù…Ù‚ÛŒØ§Ø³ 3x Ø¨Ø±Ø§ÛŒ Ú©ÛŒÙÛŒØª Ø¨Ù‡ØªØ± OCR (~216dpi Ø§Ú¯Ø± Ù¾Ø§ÛŒÙ‡ 72dpi Ø¨Ø§Ø´Ø¯)
    upscale_matrix = fitz.Matrix(3, 3)
    for page in doc:
        pix = page.get_pixmap(matrix=upscale_matrix)
        mode = "RGB" if pix.alpha == 0 else "RGBA"
        img = Image.frombytes(mode, [pix.width, pix.height], pix.samples)
        # ØªÙ„Ø§Ø´ Ø¨Ø§ ÙØ§Ø±Ø³ÛŒ+Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒØŒ Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ Ø¯Ø§Ø¯Ù‡ Ø²Ø¨Ø§Ù†ÛŒØŒ fallback Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
        try:
            text_page = pytesseract.image_to_string(img, lang="fas+eng")
        except Exception:
            text_page = pytesseract.image_to_string(img, lang="eng")
        if text_page:
            ocr_text_parts.append(text_page)
    return "\n".join(ocr_text_parts).strip()

def extract_text(pdf_path):
    # Ø§Ø¨ØªØ¯Ø§ ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ù…ØªÙ† Ø¯ÛŒØ¬ÛŒØªØ§Ù„Ø› Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ PyMuPDF ÛŒØ§ Ø¨Ø±ÙˆØ² Ø®Ø·Ø§ØŒ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
    try:
        base_text = extract_text_from_pdf(pdf_path)
    except Exception:
        base_text = ""
    if base_text and len(base_text.strip()) >= 5:
        return base_text.strip()
    # ØªÙ„Ø§Ø´ Ø¯ÙˆÙ… Ø¨Ø§ pdfminer
    miner_text = extract_text_with_pdfminer(pdf_path)
    if miner_text and len(miner_text.strip()) >= 5:
        return miner_text.strip()
    # Ø¯Ø± ØµÙˆØ±Øª ÙØ¹Ø§Ù„ Ø¨ÙˆØ¯Ù† OCR Ùˆ Ù†Ø¨ÙˆØ¯ Ù…ØªÙ† Ù…Ù†Ø§Ø³Ø¨ØŒ OCR Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯
    if _str_to_bool(os.getenv('OCR', '1'), default=True):
        try:
            return extract_text_via_ocr(pdf_path)
        except Exception:
            return (base_text or "").strip()
    return (base_text or "").strip()

def extract_text_fast(pdf_path):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø±ÛŒØ¹: Ø¨Ø¯ÙˆÙ† OCRØŒ ØªÙ„Ø§Ø´ Ø³Ø±ÛŒØ¹ Ø¨Ø§ PyMuPDF Ø³Ù¾Ø³ pdfminer."""
    # ØªÙ„Ø§Ø´ Ø³Ø±ÛŒØ¹ Ø¨Ø§ PyMuPDF
    try:
        import fitz  # PyMuPDF
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text("text")
        if text and len(text.strip()) >= 3:
            return text.strip()
    except Exception:
        pass
    # ØªÙ„Ø§Ø´ Ø¯ÙˆÙ… Ø¨Ø§ pdfminer.six
    try:
        from pdfminer.high_level import extract_text as pdfminer_extract_text
        mined = pdfminer_extract_text(pdf_path)
        return (mined or "").strip()
    except Exception:
        return ""

# ----------- LLaMA (llama-cpp) ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªÙ†Ø¨Ù„ -----------
llama_instance = None
llama_error_message = None
_llama_lock = threading.Lock()

def _str_to_bool(value, default=False):
    if value is None:
        return default
    return str(value).strip().lower() in ('1', 'true', 'yes', 'on', 'y')

def _should_use_llama() -> bool:
    # Ø¨Ù‡â€ŒØµÙˆØ±Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ ØºÛŒØ±ÙØ¹Ø§Ù„ ØªØ§ ØªÙ…Ø±Ú©Ø² Ø±ÙˆÛŒ Ollama Ø¨Ø§Ø´Ø¯
    return _str_to_bool(os.getenv('USE_LLAMA', '0'), default=False)

def _should_use_ollama() -> bool:
    return _str_to_bool(os.getenv('USE_OLLAMA', '1'), default=True)

def _normalize_text(s: str) -> str:
    if not s:
        return ''
    try:
        import re
        s = s.lower()
        s = re.sub(r"[\u200c\u200f\u200e\ufeff]", "", s)
        s = re.sub(r"[^\w\s\u0600-\u06FF]", " ", s)
        s = re.sub(r"\s+", " ", s)
        return s.strip()
    except Exception:
        return s

def _hash_key_for_question(question: str) -> str:
    try:
        import hashlib
        base = _normalize_text(question)
        return hashlib.sha256(base.encode('utf-8', errors='ignore')).hexdigest()
    except Exception:
        return question[:64]

def _score_paragraph(q_norm: str, p_norm: str) -> int:
    if not q_norm or not p_norm:
        return 0
    q_terms = set(q_norm.split())
    p_terms = set(p_norm.split())
    return len(q_terms & p_terms)

def generate_answer_locally(question: str, context: str, k: int = 3, per_snippet_chars: int = 400) -> str:
    qn = _normalize_text(question)
    if not context:
        return "Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±ØŒ Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ø²Ù…ÛŒÙ†Ù‡ Ø±Ø§ Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯."
    parts = [p for p in context.split("\n\n") if p and p.strip()]
    scored = []
    for p in parts:
        pn = _normalize_text(p)
        score = _score_paragraph(qn, pn)
        if score > 0:
            scored.append((score, p))
    if not scored:
        hint = (context[: per_snippet_chars * k] + '...') if len(context) > per_snippet_chars * k else context
        return "Ù¾Ø§Ø³Ø® Ù…Ø­Ù„ÛŒ (Ø¨Ø¯ÙˆÙ† Ù…Ø¯Ù„):\n" + hint
    scored.sort(key=lambda x: x[0], reverse=True)
    picks = [p for _s, p in scored[:k]]
    clipped = []
    for p in picks:
        clipped.append(p[:per_snippet_chars])
    result = "\n\n---\n\n".join(clipped)
    return "Ù†ØªÛŒØ¬Ù‡Ù” Ø³Ø±ÛŒØ¹ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªÙ†:\n" + result

# ----------- ØªÙˆÙ„ÛŒØ¯ Ù„Ø§ÛŒØ­Ù‡ (IRAC) -----------
def _format_brief_irac(case: Dict[str, object], citations: List[Dict[str, object]] | None) -> str:
    parties = case.get('parties') or {}
    title = case.get('title') or 'Ù¾ÛŒØ´â€ŒÙ†ÙˆÛŒØ³ Ù„Ø§ÛŒØ­Ù‡'
    facts = case.get('facts') or ''
    claims = case.get('claims') or ''
    court = case.get('court') or ''
    requests = case.get('requests') or ''
    # IRAC: Issue, Rule, Analysis, Conclusion
    lines = []
    lines.append(title)
    if court:
        lines.append(f"Ù…Ø±Ø¬Ø¹ Ø±Ø³ÛŒØ¯Ú¯ÛŒ: {court}")
    if parties:
        lines.append(f"Ø§ØµØ­Ø§Ø¨ Ø¯Ø¹ÙˆØ§: {parties}")
    lines.append("\n[Ù…Ø³Ø¦Ù„Ù‡]")
    lines.append(str(case.get('issue') or claims))
    lines.append("\n[Ù‚ÙˆØ§Ø¹Ø¯ (Ù…ÙˆØ§Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ùˆ Ø¢Ø±Ø§Ø¡ Ù…Ø±ØªØ¨Ø·)]")
    if citations:
        for c in citations:
            lines.append(f"- {c.get('snippet')}")
    else:
        lines.append("- Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ø§Ø³ØªÙ†Ø§Ø¯ÛŒ Ø«Ø¨Øª Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.")
    lines.append("\n[ØªØ­Ù„ÛŒÙ„]")
    lines.append(str(case.get('analysis') or facts))
    lines.append("\n[Ù†ØªÛŒØ¬Ù‡]")
    lines.append(str(case.get('conclusion') or requests))
    lines.append("\n[Ù¾ÛŒÙˆØ³Øªâ€ŒÙ‡Ø§]")
    att = case.get('attachments') or []
    if att:
        for i, a in enumerate(att, 1):
            lines.append(f"Ø¶Ù…ÛŒÙ…Ù‡ {i}: {a}")
    return "\n".join([str(x) for x in lines])

@app.route('/draft', methods=['POST'])
def create_draft():
    data = request.get_json(silent=True) or {}
    question = data.get('prompt') or data.get('question') or 'ØªÙ‡ÛŒÙ‡ Ù¾ÛŒØ´â€ŒÙ†ÙˆÛŒØ³ Ù„Ø§ÛŒØ­Ù‡'
    case = data.get('case') or {}
    # Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø³ØªÙ†Ø§Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù†ÙˆØ§Ù†/Ø§Ø¯Ø¹Ø§/ÙˆÙ‚Ø§ÛŒØ¹
    needle = " ".join([str(case.get('title') or ''), str(case.get('claims') or ''), str(case.get('facts') or '')]).strip()
    citations = []
    try:
        cid = request.cookies.get('client_id')
        if cid and needle:
            _ensure_session_index(cid)
            retrieved = _retrieve_paragraphs(needle, cid, top_k=6)
            total_score = sum(int(r.get('score') or 0) for r in retrieved)
            strong = [r for r in retrieved if (r.get('score') or 0) >= 2]
            if retrieved and (len(strong) >= 1 or total_score >= 3):
                citations = [{'score': int(r['score']), 'snippet': (r['snippet'][:500] + '...') if len(r['snippet']) > 500 else r['snippet']} for r in retrieved]
                context = "\n\n---\n\n".join([c['snippet'] for c in citations])
            else:
                context = ''
        else:
            context = ''
    except Exception:
        context = ''
    # ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ù„Ø§ÛŒØ­Ù‡: Ø§Ú¯Ø± Ù…Ø¯Ù„ Ù‡Ø³Øª Ø§Ø² generate_answer Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŒ ÙˆÚ¯Ø±Ù†Ù‡ ÙØ±Ù…Øª IRAC Ù…Ø­Ù„ÛŒ
    if context:
        prompt = f"Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³ØªÙ†Ø§Ø¯Ø§Øª Ø²ÛŒØ± Ù„Ø§ÛŒØ­Ù‡â€ŒØ§ÛŒ Ø±Ø³Ù…ÛŒ Ùˆ Ú©ÙˆØªØ§Ù‡ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ù…Ø³Ø¦Ù„Ù‡/Ù‚ÙˆØ§Ø¹Ø¯/ØªØ­Ù„ÛŒÙ„/Ù†ØªÛŒØ¬Ù‡ Ø¨Ù†ÙˆÛŒØ³:\n\n{context}\n\n[Ù…ÙˆØ¶ÙˆØ¹ Ù¾Ø±ÙˆÙ†Ø¯Ù‡]\n{needle}\n\n[Ù¾Ø§Ø³Ø®]"
        draft_text = generate_answer(prompt, context)
    else:
        draft_text = _format_brief_irac(case, citations)
    return jsonify({ 'draft': draft_text, 'citations': citations })

@app.route('/draft/docx', methods=['POST'])
def create_draft_docx():
    try:
        from docx import Document
    except Exception:
        return jsonify({'error': 'Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ python-docx Ù†ØµØ¨ Ù†ÛŒØ³Øª. Ù†ØµØ¨: pip install python-docx'}), 400
    data = request.get_json(silent=True) or {}
    draft = data.get('draft') or ''
    if not draft:
        return jsonify({'error': 'Ù…ØªÙ† Ù„Ø§ÛŒØ­Ù‡ Ø§Ø±Ø³Ø§Ù„ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª'}), 400
    doc = Document()
    for line in str(draft).split('\n'):
        doc.add_paragraph(line)
    bio = io.BytesIO()
    doc.save(bio)
    bio.seek(0)
    resp = make_response(bio.read())
    resp.headers['Content-Type'] = 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
    resp.headers['Content-Disposition'] = 'attachment; filename="draft.docx"'
    return resp

def load_llama():
    global llama_instance, llama_error_message
    if llama_instance is not None:
        return llama_instance
    with _llama_lock:
        if llama_instance is not None:
            return llama_instance
        try:
            from llama_cpp import Llama
        except Exception as exc:
            llama_error_message = "Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ llama-cpp-python Ù†ØµØ¨ Ù†ÛŒØ³Øª. Ù†ØµØ¨: pip install llama-cpp-python"
            return None

        model_path = os.getenv('LLAMA_MODEL')
        if not model_path or not os.path.exists(model_path):
            llama_error_message = "Ù…Ø³ÛŒØ± Ù…Ø¯Ù„ LLAMA_MODEL ØªÙ†Ø¸ÛŒÙ…/Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª. ÛŒÚ© ÙØ§ÛŒÙ„ .gguf Ù…Ø¹ØªØ¨Ø± Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯."
            return None

        def _as_int(env_name, default_val):
            try:
                return int(os.getenv(env_name, str(default_val)))
            except ValueError:
                return default_val

        n_ctx = _as_int('LLAMA_CTX', 4096)
        n_threads = _as_int('LLAMA_THREADS', max(1, (os.cpu_count() or 4)))
        n_gpu_layers = _as_int('LLAMA_N_GPU_LAYERS', 0)

        try:
            kwargs = {
                'model_path': model_path,
                'n_ctx': n_ctx,
                'n_threads': n_threads
            }
            if n_gpu_layers > 0:
                kwargs['n_gpu_layers'] = n_gpu_layers
            llama_instance = Llama(**kwargs)
            llama_error_message = None
            return llama_instance
        except Exception as exc:
            llama_error_message = f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {exc}"
            return None

def generate_answer_with_llama(question, context):
    model = load_llama()
    if model is None:
        fallback = (
            "Ù…Ø¯Ù„ Ù…Ø­Ù„ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ/Ù†ØµØ¨ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª. "
            + (f"Ø¬Ø²Ø¦ÛŒØ§Øª: {llama_error_message}" if llama_error_message else "")
            + "\n\n" 
            + f"ðŸ” ØªØ­Ù„ÛŒÙ„ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªÙ† Ù…ÙˆØ¬ÙˆØ¯:\n\n{question}\n\nðŸ‘‰ Ù¾Ø§Ø³Ø® Ù†Ù…ÙˆÙ†Ù‡: Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªÙ† Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ØŒ ØªØ´Ø®ÛŒØµ Ø¯Ù‚ÛŒÙ‚ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ø¨ÛŒØ´ØªØ± Ø¯Ø§Ø±Ø¯."
        )
        return fallback

    def _as_float(env_name, default_val):
        try:
            return float(os.getenv(env_name, str(default_val)))
        except ValueError:
            return default_val

    def _as_int(env_name, default_val):
        try:
            return int(os.getenv(env_name, str(default_val)))
        except ValueError:
            return default_val

    fast = _is_fast_mode()
    temperature = _as_float('LLAMA_TEMP', 0.15 if fast else 0.2)
    max_tokens = _as_int('LLAMA_MAX_TOKENS', 256 if fast else 512)
    top_p = _as_float('LLAMA_TOP_P', 0.9 if fast else 0.95)

    prompt = (
        "Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ø­Ù‚ÙˆÙ‚ÛŒ ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒØ¯. Ø¨Ø§ ØªÚ©ÛŒÙ‡ Ø¨Ø± Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø²ÛŒØ±ØŒ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ùˆ Ú©ÙˆØªØ§Ù‡ Ø¨Ø¯Ù‡.\n\n"
        + "[Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡]" + "\n" + context + "\n\n"
        + "[Ø³Ø¤Ø§Ù„]" + "\n" + question + "\n\n"
        + "[Ù¾Ø§Ø³Ø®]"
    )

    try:
        resp = model.create_completion(
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=top_p,
        )
        text = resp.get('choices', [{}])[0].get('text', '').strip()
        return text or "Ù¾Ø§Ø³Ø®ÛŒ Ø§Ø² Ù…Ø¯Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯."
    except Exception as exc:
        return f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø§Ø² Ù…Ø¯Ù„: {exc}"

def generate_answer_with_ollama(question, context):
    import json
    import urllib.request
    import urllib.error

    host = os.getenv('OLLAMA_HOST', 'http://127.0.0.1:11434')
    model = os.getenv('OLLAMA_MODEL', 'gemma3:4b')
    url = host.rstrip('/') + '/api/generate'

    prompt = (
        "Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ø­Ù‚ÙˆÙ‚ÛŒ ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒØ¯. Ø¨Ø§ ØªÚ©ÛŒÙ‡ Ø¨Ø± Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø²ÛŒØ±ØŒ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ùˆ Ú©ÙˆØªØ§Ù‡ Ø¨Ø¯Ù‡.\n\n"
        + "[Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡]" + "\n" + context + "\n\n"
        + "[Ø³Ø¤Ø§Ù„]" + "\n" + question + "\n\n"
        + "[Ù¾Ø§Ø³Ø®]"
    )

    fast = _is_fast_mode()
    payload = {
        'model': model,
        'prompt': prompt,
        'stream': False,
        'options': {
            'temperature': float(os.getenv('OLLAMA_TEMP', '0.15' if fast else '0.2')),
            'top_p': float(os.getenv('OLLAMA_TOP_P', '0.9' if fast else '0.95')),
            'num_predict': int(os.getenv('OLLAMA_MAX_TOKENS', '256' if fast else '512')),
        }
    }

    def list_installed_models():
        try:
            tags_url = host.rstrip('/') + '/api/tags'
            with urllib.request.urlopen(tags_url, timeout=5 if _is_fast_mode() else 10) as resp:
                tags = json.loads(resp.read().decode('utf-8')) or {}
                models = tags.get('models') or []
                names = []
                for m in models:
                    name = m.get('name') or m.get('model')
                    if name:
                        names.append(name)
                return names
        except Exception:
            return []

    def is_reachable():
        try:
            tags_url = host.rstrip('/') + '/api/tags'
            with urllib.request.urlopen(tags_url, timeout=3) as resp:
                _ = resp.read()
                return True
        except Exception:
            return False

    def do_chat(use_model):
        chat_url = host.rstrip('/') + '/api/chat'
        messages = [
            { 'role': 'system', 'content': 'Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ø­Ù‚ÙˆÙ‚ÛŒ ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒØ¯. Ø¨Ø§ ØªÚ©ÛŒÙ‡ Ø¨Ø± Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø²ÛŒØ±ØŒ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ùˆ Ú©ÙˆØªØ§Ù‡ Ø¨Ø¯Ù‡.' },
            { 'role': 'user', 'content': f"[Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡]\n{context}\n\n[Ø³Ø¤Ø§Ù„]\n{question}" }
        ]
        payload_chat = {
            'model': use_model,
            'messages': messages,
            'stream': False,
            'options': {
                'temperature': float(os.getenv('OLLAMA_TEMP', '0.2')),
                'top_p': float(os.getenv('OLLAMA_TOP_P', '0.95')),
                'num_predict': int(os.getenv('OLLAMA_MAX_TOKENS', '512')),
            }
        }
        data_chat = json.dumps(payload_chat).encode('utf-8')
        req_chat = urllib.request.Request(chat_url, data=data_chat, headers={'Content-Type': 'application/json'})
        try:
            try:
                timeout_s = float(os.getenv('OLLAMA_TIMEOUT', '45' if fast else '120'))
            except ValueError:
                timeout_s = 45.0 if fast else 120.0
            with urllib.request.urlopen(req_chat, timeout=timeout_s) as resp:
                res = json.loads(resp.read().decode('utf-8'))
                # Ø³Ø§Ø®ØªØ§Ø± chat: res['message']['content']
                msg = (res.get('message') or {}).get('content', '')
                return msg.strip() or "Ù¾Ø§Ø³Ø®ÛŒ Ø§Ø² Ù…Ø¯Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯."
        except Exception as exc:
            return f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ollama (chat): {exc}"

    def do_generate(use_model, tried_fallback=False):
        payload_local = dict(payload)
        payload_local['model'] = use_model
        data_local = json.dumps(payload_local).encode('utf-8')
        req_local = urllib.request.Request(url, data=data_local, headers={'Content-Type': 'application/json'})
        try:
            try:
                timeout_s = float(os.getenv('OLLAMA_TIMEOUT', '45' if fast else '120'))
            except ValueError:
                timeout_s = 45.0 if fast else 120.0
            with urllib.request.urlopen(req_local, timeout=timeout_s) as resp:
                res = json.loads(resp.read().decode('utf-8'))
                text = (res.get('response') or '').strip()
                return text or "Ù¾Ø§Ø³Ø®ÛŒ Ø§Ø² Ù…Ø¯Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯."
        except urllib.error.HTTPError as http_exc:
            # Ø§Ú¯Ø± Ù…Ø¯Ù„ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ (404)ØŒ Ø±ÙˆÛŒ ÛŒÚ© Ù…Ø¯Ù„ Ù†ØµØ¨â€ŒØ´Ø¯Ù‡ Ø³ÙˆÛŒÛŒÚ† Ú©Ù†
            if http_exc.code == 404:
                # Ø§Ø¨ØªØ¯Ø§ ØªÙ„Ø§Ø´ Ø¨Ø§ /api/chat Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø®ÛŒ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡â€ŒÙ‡Ø§ (Ù…Ø«Ù„ qwen3/deepseek)
                chat_ans = do_chat(use_model)
                if chat_ans and not chat_ans.startswith('Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ollama'):
                    return chat_ans
                installed = list_installed_models()
                fallback = os.getenv('OLLAMA_FALLBACK_MODEL')
                pick = fallback or ('gemma3:4b' if 'gemma3:4b' in installed else ('llama3.2' if 'llama3.2' in installed else (installed[0] if installed else None)))
                if pick and pick != use_model:
                    try:
                        return do_generate(pick, tried_fallback=True)
                    except Exception:
                        pass
                return f"Ù…Ø¯Ù„ '{use_model}' Ø¯Ø± Ollama Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. ÛŒÚ© Ù…Ø¯Ù„ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯ (Ù…Ø«Ù„Ø§Ù‹: ollama pull {use_model}) ÛŒØ§ Ù…ØªØºÛŒØ± OLLAMA_MODEL Ø±Ø§ Ø±ÙˆÛŒ ÛŒÚ©ÛŒ Ø§Ø² Ù†ØµØ¨â€ŒØ´Ø¯Ù‡â€ŒÙ‡Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯: {installed}"
            return f"Ø®Ø·Ø§ÛŒ HTTP Ø§Ø² Ollama: {http_exc}"
        except Exception as exc:
            # Ø¨Ø±Ø®ÛŒ Ù…Ø­ÛŒØ·â€ŒÙ‡Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ù‡ Ø¬Ø§ÛŒ HTTPErrorØŒ Exception Ø¹Ù…ÙˆÙ…ÛŒ Ø¨Ø§ Ù…ØªÙ† 404 Ø¨Ø¯Ù‡Ù†Ø¯
            msg = str(exc)
            if (('404' in msg) or ('Not Found' in msg)) and not tried_fallback:
                chat_ans = do_chat(use_model)
                if chat_ans and not chat_ans.startswith('Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ollama'):
                    return chat_ans
                installed = list_installed_models()
                fallback = os.getenv('OLLAMA_FALLBACK_MODEL')
                pick = fallback or ('gemma3:4b' if 'gemma3:4b' in installed else ('llama3.2' if 'llama3.2' in installed else (installed[0] if installed else None)))
                if pick and pick != use_model:
                    try:
                        return do_generate(pick, tried_fallback=True)
                    except Exception:
                        pass
            return f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ollama: {exc}"

    # Ø§Ú¯Ø± Ø³Ø±ÙˆÛŒØ³ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³ØªØŒ Ø³Ø±ÛŒØ¹Ø§Ù‹ Ø®Ø§Ø±Ø¬ Ø´Ùˆ (Ø¨Ø¯ÙˆÙ† ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ù‡ Ù†ØµØ¨ Ù…Ø¯Ù„)
    if not is_reachable():
        return 'Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ollama: Ø³Ø±ÙˆÛŒØ³ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª'

    # Ø§Ø¨ØªØ¯Ø§ chat Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (Ø¨Ø±Ø§ÛŒ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø«Ù„ gemma/deepseek/qwen)
    chat_first = do_chat(model)
    if chat_first and not chat_first.startswith('Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ollama'):
        return chat_first
    # Ø¯Ø± ØµÙˆØ±Øª Ø´Ú©Ø³Øª chatØŒ generate Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø¨Ø§ Ù…Ú©Ø§Ù†ÛŒØ²Ù… fallback
    return do_generate(model)

def _should_use_deepseek() -> bool:
    key = os.getenv('DEEPSEEK_API_KEY')
    if key and len(key.strip()) > 10:
        return True
    return _str_to_bool(os.getenv('USE_DEEPSEEK', '0'), default=False)

def generate_answer_with_deepseek(question: str, context: str) -> str:
    try:
        import requests  # type: ignore
    except Exception:
        return 'Ø®Ø·Ø§ Ø¯Ø± DeepSeek: Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ requests Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª'

    api_key = os.getenv('DEEPSEEK_API_KEY', '').strip()
    model = os.getenv('DEEPSEEK_MODEL', 'deepseek-chat').strip() or 'deepseek-chat'
    if not api_key:
        return 'Ø®Ø·Ø§ Ø¯Ø± DeepSeek: Ú©Ù„ÛŒØ¯ API ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª'

    fast = _is_fast_mode()
    try:
        temperature = float(os.getenv('DEEPSEEK_TEMP', '0.15' if fast else '0.2'))
    except Exception:
        temperature = 0.15 if fast else 0.2
    try:
        top_p = float(os.getenv('DEEPSEEK_TOP_P', '0.9' if fast else '0.95'))
    except Exception:
        top_p = 0.9 if fast else 0.95
    try:
        max_tokens = int(os.getenv('DEEPSEEK_MAX_TOKENS', '256' if fast else '512'))
    except Exception:
        max_tokens = 256 if fast else 512

    url = 'https://api.deepseek.com/chat/completions'
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json',
    }
    payload = {
        'model': model,
        'messages': [
            { 'role': 'system', 'content': 'Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ø­Ù‚ÙˆÙ‚ÛŒ ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒØ¯. Ø¨Ø§ ØªÚ©ÛŒÙ‡ Ø¨Ø± Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ø²ÛŒØ±ØŒ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ùˆ Ú©ÙˆØªØ§Ù‡ Ø¨Ø¯Ù‡.' },
            { 'role': 'user', 'content': f"[Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡]\n{context}\n\n[Ø³Ø¤Ø§Ù„]\n{question}" },
        ],
        'temperature': temperature,
        'top_p': top_p,
        'max_tokens': max_tokens,
        'stream': False,
    }
    try:
        timeout_s = float(os.getenv('DEEPSEEK_TIMEOUT', '45' if fast else '120'))
    except Exception:
        timeout_s = 45.0 if fast else 120.0

    ts0 = time.perf_counter()
    try:
        resp = requests.post(url, json=payload, headers=headers, timeout=timeout_s)
        took = int((time.perf_counter() - ts0) * 1000)
        MODEL_STATUS.update({'engine':'deepseek','latency_ms':took,'ts':int(time.time())})
        if resp.status_code >= 400:
            MODEL_STATUS.update({'error': f'HTTP {resp.status_code}'})
            return f"Ø®Ø·Ø§ Ø¯Ø± DeepSeek: HTTP {resp.status_code} - {resp.text[:200]}"
        data = resp.json()
        choices = data.get('choices') or []
        if not choices:
            MODEL_STATUS.update({'error':'no_choices'})
            return 'Ù¾Ø§Ø³Ø®ÛŒ Ø§Ø² DeepSeek Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯.'
        content = ((choices[0] or {}).get('message') or {}).get('content', '')
        MODEL_STATUS.update({'error': ''})
        return content.strip() or 'Ù¾Ø§Ø³Ø®ÛŒ Ø§Ø² DeepSeek Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯.'
    except Exception as exc:
        MODEL_STATUS.update({'engine':'deepseek','error': str(exc), 'ts': int(time.time())})
        return f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ DeepSeek: {exc}"

def generate_answer(question, context):
    # Ø§Ø¨ØªØ¯Ø§ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡Ù” Ø§Ù†Ø³Ø§Ù†ÛŒ Ø±Ø§ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    try:
        key = _hash_key_for_question(question)
        curated = CURATED_ANSWERS.get(key)
        if curated and curated.get('answer'):
            return str(curated['answer'])
    except Exception:
        pass
    # Ø­Ø§Ù„Øª Ø¨Ø¯ÙˆÙ† Ù…Ø¯Ù„ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ): ÙÙ‚Ø· Ù¾Ø§Ø³Ø® Ù…Ø­Ù„ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´ÙˆØ¯
    if _str_to_bool(os.getenv('NO_MODELS', '0'), default=False):
        return generate_answer_locally(question, context)

    # ØªÙ„Ø§Ø´ Ø¨Ø§ DeepSeek (Ø¯Ø± ØµÙˆØ±Øª Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ)
    deepseek_ans = None
    if _should_use_deepseek():
        deepseek_ans = generate_answer_with_deepseek(question, context)
        if isinstance(deepseek_ans, str) and not deepseek_ans.startswith('Ø®Ø·Ø§ Ø¯Ø±') and not deepseek_ans.startswith('Ù¾Ø§Ø³Ø®ÛŒ Ø§Ø² DeepSeek'):
            return deepseek_ans

    # ØªÙ„Ø§Ø´ Ø¨Ø§ llama-cpp (Ø¯Ø± ØµÙˆØ±Øª ÙØ¹Ø§Ù„ Ø¨ÙˆØ¯Ù†)
    llama_ans = None
    if _should_use_llama():
        llama_ans = generate_answer_with_llama(question, context)
        if isinstance(llama_ans, str) and not llama_ans.startswith('Ù…Ø¯Ù„ Ù…Ø­Ù„ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ') and not llama_ans.startswith('Ø®Ø·Ø§ Ø¯Ø±'):
            return llama_ans

    # ØªÙ„Ø§Ø´ Ø¨Ø§ Ollama
    ollama_ans = None
    if _should_use_ollama():
        ollama_ans = generate_answer_with_ollama(question, context)
        if isinstance(ollama_ans, str) and not ollama_ans.startswith('Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ollama') and not ollama_ans.startswith('Ø®Ø·Ø§ÛŒ HTTP') and not ollama_ans.startswith('Ø®Ø·Ø§ Ø¯Ø±'):
            return ollama_ans

    # Ø§Ú¯Ø± Ù‡Ø± Ø¯Ùˆ Ù…Ø³ÛŒØ± Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯ØŒ Ù¾Ø§Ø³Ø® Ù…Ø­Ù„ÛŒ Ø¨Ø¯Ù‡ Ùˆ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ Ø±Ø§ Ø¶Ù…ÛŒÙ…Ù‡ Ú©Ù†
    local = generate_answer_locally(question, context)
    debug = []
    if isinstance(deepseek_ans, str) and (deepseek_ans.startswith('Ø®Ø·Ø§ Ø¯Ø±') or deepseek_ans.startswith('Ù¾Ø§Ø³Ø®ÛŒ Ø§Ø² DeepSeek')):
        debug.append(deepseek_ans)
    if isinstance(llama_ans, str) and (llama_ans.startswith('Ù…Ø¯Ù„ Ù…Ø­Ù„ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ') or llama_ans.startswith('Ø®Ø·Ø§ Ø¯Ø±')):
        debug.append(llama_ans)
    if isinstance(ollama_ans, str) and (ollama_ans.startswith('Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ollama') or ollama_ans.startswith('Ø®Ø·Ø§ÛŒ HTTP') or ollama_ans.startswith('Ø®Ø·Ø§ Ø¯Ø±')):
        debug.append(ollama_ans)
    return local + ("\n\n" + "\n".join(debug) if debug else '')

# ----------- ØµÙØ­Ù‡ ÙˆØ¨ Ø³Ø§Ø¯Ù‡ -----------
@app.route('/', methods=['GET'])
def index():
    # Ø®ÙˆØ§Ù†Ø¯Ù† Ø®ÙˆØ¯Ú©Ø§Ø± ÙÙ‚Ø· Ø§Ú¯Ø± Ù‡Ù†ÙˆØ² Ú†ÛŒØ²ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø³Ø´Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
    client_id = request.cookies.get('client_id') or uuid.uuid4().hex
    with _context_lock:
        existing = SESSION_CONTEXTS.get(client_id) or []
        already_processed = SESSION_PROCESSED.get(client_id) or set()
    saved_texts: List[str] = []
    previews: List[str] = []
    total_files = 0
    if not existing and not already_processed:
        saved_texts, previews, total_files = ingest_pdfs_from_dir(DEFAULT_PDF_DIR, recursive=True, client_id=client_id)
    if saved_texts:
        _append_session_context(client_id, saved_texts)
    combined_preview = '\n\n---\n\n'.join(previews) if previews else ''

    html = render_template('index.html', combined_preview=combined_preview, total_files=total_files)

    resp = make_response(html)
    # Ø³Øª Ú©Ø±Ø¯Ù† Ú©ÙˆÚ©ÛŒ Ø³Ø´Ù† Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯
    if not request.cookies.get('client_id'):
        resp.set_cookie('client_id', client_id, max_age=30*24*3600, httponly=False, samesite='Lax')
    return resp

# Ø¢Ù¾Ù„ÙˆØ¯ Ø­Ø°Ù Ø´Ø¯Ù‡ Ø§Ø³Øª

# ----------- Ø®ÙˆØ§Ù†Ø¯Ù† PDFÙ‡Ø§ Ø§Ø² Ù¾ÙˆØ´Ù‡Ù” Ø±ÙˆÛŒ Ø³Ø±ÙˆØ± -----------
@app.route('/ingest-dir', methods=['POST'])
def ingest_directory():
    data = request.get_json(silent=True) or {}
    dir_path = data.get('dir')
    recursive = bool(data.get('recursive', True))
    # Ø§Ù†Ø¯Ø§Ø²Ù‡Ù” Ø¨Ú† (Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÚ©Ù‡â€ŒØªÚ©Ù‡)
    try:
        req_max = int(data.get('max_files', 0))
    except Exception:
        req_max = 0
    # Ø§Ú¯Ø± dir Ø§Ø±Ø³Ø§Ù„ Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ØŒ Ø§Ø² Ù¾ÙˆØ´Ù‡Ù” Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    if not dir_path:
        dir_path = DEFAULT_PDF_DIR
        recursive = True
    if not dir_path or not isinstance(dir_path, str):
        return jsonify({'error': 'Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡ Ø§Ø±Ø³Ø§Ù„ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª'}), 400
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…Ø³ÛŒØ± (Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ/ÙˆÛŒÙ†Ø¯ÙˆØ²)
    dir_path = os.path.expanduser(dir_path)
    if not os.path.isabs(dir_path):
        dir_path = os.path.abspath(os.path.join(BASE_DIR, dir_path))
    if not os.path.isdir(dir_path):
        return jsonify({'error': 'Ù¾ÙˆØ´Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯'}), 400

    client_id = _get_or_create_client_id()
    saved_texts: List[str] = []
    previews: List[str] = []
    total_files = 0
    # Ø­Ø¯Ø§Ú©Ø«Ø± Ù‡Ø± ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ
    max_files = 200
    if req_max and req_max > 0:
        max_files = max(1, min(req_max, 500))

    def iter_pdfs(path):
        if recursive:
            for root, _dirs, files in os.walk(path):
                for name in files:
                    if name.lower().endswith('.pdf'):
                        yield os.path.join(root, name)
        else:
            for name in os.listdir(path):
                fp = os.path.join(path, name)
                if os.path.isfile(fp) and name.lower().endswith('.pdf'):
                    yield fp

    fast = bool(data.get('fast', False))
    for fp in iter_pdfs(dir_path):
        try:
            extracted_text = extract_text_fast(fp) if fast else extract_text(fp)
        except Exception:
            extracted_text = ''
        if extracted_text:
            saved_texts.append(extracted_text)
            pv = (extracted_text[:500] + '...') if len(extracted_text) > 500 else extracted_text
            previews.append(pv)
            total_files += 1
            if total_files >= max_files:
                break

    if saved_texts:
        _append_session_context(client_id, saved_texts)
        # Ù¾Ø³ Ø§Ø² Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ØŒ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø§ÛŒÙ† Ø³Ø´Ù† Ø±Ø§ Ù†ÛŒØ² ØªØ§Ø²Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒÙ…
        try:
            _ensure_session_index(client_id)
        except Exception:
            pass

    combined_preview = '\n\n---\n\n'.join(previews)
    resp = jsonify({
        'message': 'ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø§Ø² Ù¾ÙˆØ´Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù†Ø¯.',
        'dir': dir_path,
        'total_files': total_files,
        'previews': previews,
        'combined_preview': combined_preview
    })
    response = make_response(resp)
    response.set_cookie('client_id', client_id, max_age=30*24*3600, httponly=False, samesite='Lax')
    return response

@app.route('/ingest-prewarm', methods=['POST'])
def ingest_prewarm():
    """Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÚ©Ù‡â€ŒØªÚ©Ù‡ Ø¯Ø± Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ Ø³Ø±ÛŒØ¹."""
    data = request.get_json(silent=True) or {}
    dir_path = data.get('dir') or DEFAULT_PDF_DIR
    recursive = bool(data.get('recursive', True))
    try:
        batch_size = int(data.get('max_files', 20))
    except Exception:
        batch_size = 20
    fast = _str_to_bool(str(data.get('fast', '1')), default=True)

    client_id = _get_or_create_client_id()

    # Ù…Ø­Ø§Ø³Ø¨Ù‡Ù” Ú©Ù„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‡Ø¯Ù (Ø¨Ù‡â€ŒØ¬Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ø§ÛŒÙ† Ø³Ø´Ù†)
    def _count_total_targets() -> int:
        try:
            proc: Set[str] = set()
            with _context_lock:
                proc = set(SESSION_PROCESSED.get(client_id) or set())
            cnt = 0
            if recursive:
                for root, _dirs, files in os.walk(dir_path):
                    for name in files:
                        if name.lower().endswith('.pdf'):
                            fp = os.path.abspath(os.path.join(root, name))
                            if fp not in proc:
                                cnt += 1
            else:
                for name in os.listdir(dir_path):
                    fp = os.path.abspath(os.path.join(dir_path, name))
                    if os.path.isfile(fp) and name.lower().endswith('.pdf') and fp not in proc:
                        cnt += 1
            return cnt
        except Exception:
            return 0

    total_targets = _count_total_targets()
    with _context_lock:
        PREWARM_PROGRESS[client_id] = {
            'running': True,
            'done': 0,
            'total': total_targets,
            'dir': dir_path,
            'fast': fast,
        }

    def _worker():
        try:
            while True:
                _texts, _prev, count = ingest_pdfs_from_dir(dir_path, recursive=recursive, client_id=client_id, fast=fast, max_files=batch_size)
                if count <= 0:
                    with _context_lock:
                        st = PREWARM_PROGRESS.get(client_id) or {}
                        st['running'] = False
                        PREWARM_PROGRESS[client_id] = st
                    break
                with _context_lock:
                    st = PREWARM_PROGRESS.get(client_id) or {}
                    st_done = int(st.get('done') or 0) + int(count)
                    st['done'] = st_done
                    PREWARM_PROGRESS[client_id] = st
        except Exception:
            pass

    t = threading.Thread(target=_worker, daemon=True)
    t.start()
    resp = jsonify({'message': 'Ù¾ÛŒØ´â€ŒØ¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¢ØºØ§Ø² Ø´Ø¯', 'client_id': client_id})
    response = make_response(resp)
    response.set_cookie('client_id', client_id, max_age=30*24*3600, httponly=False, samesite='Lax')
    return response

@app.route('/ingest-progress', methods=['GET'])
def ingest_progress():
    client_id = request.cookies.get('client_id')
    with _context_lock:
        st = PREWARM_PROGRESS.get(client_id or '')
    if not st:
        return jsonify({'running': False, 'done': 0, 'total': 0, 'percent': 0})
    done = int(st.get('done') or 0)
    total = int(st.get('total') or 0)
    percent = int((done * 100) / total) if total > 0 else (100 if not st.get('running') else 0)
    return jsonify({
        'running': bool(st.get('running')),
        'done': done,
        'total': total,
        'percent': percent,
        'dir': st.get('dir'),
        'fast': st.get('fast'),
    })

# Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ÛŒ Ø­Ø¬Ù… Ø²ÛŒØ§Ø¯ (413) Ø¨Ù‡ ØµÙˆØ±Øª JSON
@app.errorhandler(413)
def request_entity_too_large(_):
    return jsonify({'error': 'Ø­Ø¬Ù… ÙØ§ÛŒÙ„ Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ Ù…Ø¬Ø§Ø² Ø§Ø³Øª (Ø­Ø¯Ø§Ú©Ø«Ø± Û± Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª).'}), 413

# Ù¾Ø§Ø³Ø® JSON Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡Ù” Ø®Ø·Ø§Ù‡Ø§ÛŒ HTTP (Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù¾Ø§Ø³Ø® HTML Ú©Ù‡ Ø¨Ø§Ø¹Ø« Failed to fetch Ø¯Ø± fetch Ù…ÛŒâ€ŒØ´ÙˆØ¯)
@app.errorhandler(HTTPException)
def handle_http_exception(e: HTTPException):
    response = e.get_response()
    return jsonify({'error': e.description or 'Ø®Ø·Ø§ÛŒ Ø³Ø±ÙˆØ±'}), e.code

# Ø³Ù„Ø§Ù…Øª
@app.get('/healthz')
def healthz():
    return 'ok', 200

# ----------- Ù…Ø³ÛŒØ± Ù…ØªÙ† Ú©Ø§Ù…Ù„ Ø³Ø´Ù† -----------
@app.route('/context', methods=['GET'])
def get_full_context():
    client_id = request.cookies.get('client_id')
    texts: List[str] = []
    if client_id:
        with _context_lock:
            texts = SESSION_CONTEXTS.get(client_id) or []
    # Ø§Ú¯Ø± Ø³Ø´Ù† Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯ØŒ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ø² Ù¾ÙˆØ´Ù‡Ù” Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø®ÙˆØ§Ù†
    if not texts:
        cid = client_id or uuid.uuid4().hex
        logger.info("Context empty; auto-ingest from default: %s", DEFAULT_PDF_DIR)
        saved_texts, _previews, _total = ingest_pdfs_from_dir(DEFAULT_PDF_DIR, recursive=True, client_id=cid)
        if saved_texts:
            _append_session_context(cid, saved_texts)
            client_id = cid
            with _context_lock:
                texts = SESSION_CONTEXTS.get(client_id) or []
    combined = "\n\n---\n\n".join(texts)
    if request.args.get('download'):
        # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨Ù‡ ØµÙˆØ±Øª ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ
        resp = make_response(combined)
        resp.headers['Content-Type'] = 'text/plain; charset=utf-8'
        resp.headers['Content-Disposition'] = 'attachment; filename="context.txt"'
        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø³Øªâ€ŒØ´Ø¯Ù† Ú©ÙˆÚ©ÛŒ Ø³Ø´Ù†
        if client_id and not request.cookies.get('client_id'):
            resp.set_cookie('client_id', client_id, max_age=30*24*3600, httponly=False, samesite='Lax')
        return resp
    data = {'combined': combined, 'count': len(texts)}
    resp_json = make_response(jsonify(data))
    if client_id and not request.cookies.get('client_id'):
        resp_json.set_cookie('client_id', client_id, max_age=30*24*3600, httponly=False, samesite='Lax')
    return resp_json

# ----------- Ù…Ø³ÛŒØ± Ù¾Ø±Ø³ÛŒØ¯Ù† Ø³Ø¤Ø§Ù„ -----------
@app.route('/ask', methods=['POST'])
@limiter.limit(os.getenv('RATE_LIMIT_ASK', '60 per minute'))
def ask_question():
    data = request.get_json()
    question = data.get('question')
    context = data.get('context')

    if not question:
        return jsonify({'error': 'Ø³Ø¤Ø§Ù„ Ø§Ø±Ø³Ø§Ù„ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª'}), 400

    # Ø§Ú¯Ø± Context Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯ØŒ Ø§Ø² Ø­Ø§ÙØ¸Ù‡Ù” Ø³Ø´Ù† (Ø¨Ø± Ù¾Ø§ÛŒÙ‡Ù” Ú©ÙˆÚ©ÛŒ client_id) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    if not context or not context.strip():
        client_id = request.cookies.get('client_id')
        if client_id:
            with _context_lock:
                texts = SESSION_CONTEXTS.get(client_id) or []
            if texts:
                # Ù…Ø­Ø¯ÙˆØ¯Ø³Ø§Ø²ÛŒ Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±
                if _is_fast_mode():
                    context = _build_limited_context(texts, per_text_limit=1000, total_limit=8000)
                else:
                    context = _build_limited_context(texts, per_text_limit=2000, total_limit=15000)
        #Fallback: Ø§Ú¯Ø± Ù‡Ù†ÙˆØ² Ø®Ø§Ù„ÛŒ Ø§Ø³ØªØŒ ÛŒÚ©â€ŒØ¨Ø§Ø± ingest Ø§Ø² Ù¾ÙˆØ´Ù‡Ù” Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡
        if not context or not context.strip():
            cid = client_id or uuid.uuid4().hex
            logger.info("/ask fallback ingest from default: %s", DEFAULT_PDF_DIR)
            saved_texts, _previews, _total = ingest_pdfs_from_dir(DEFAULT_PDF_DIR, recursive=True, client_id=cid)
            if saved_texts:
                _append_session_context(cid, saved_texts)
                with _context_lock:
                    texts = SESSION_CONTEXTS.get(cid) or []
                if texts:
                    if _is_fast_mode():
                        context = _build_limited_context(texts, per_text_limit=1000, total_limit=8000)
                    else:
                        context = _build_limited_context(texts, per_text_limit=2000, total_limit=15000)
                # Ø³Øªâ€ŒÚ©Ø±Ø¯Ù† Ú©ÙˆÚ©ÛŒ Ø³Ø´Ù† Ø¯Ø± Ù¾Ø§Ø³Ø®
                resp = jsonify({'error': None})
                resp.set_cookie('client_id', cid, max_age=30*24*3600, httponly=False, samesite='Lax')
                # Ø§Ø¯Ø§Ù…Ù‡Ù” Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø§Ø³Ø® Ø¨Ø§ context Ù¾Ø± Ø´Ø¯Ù‡Ø› Ø¨Ø¯ÙˆÙ† Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø§ÛŒÙ† resp Ù…ÙˆÙ‚Øª
    if not context or not context.strip():
        # Ø§Ú¯Ø± Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ù…Ù‡ÛŒØ§ Ù†Ø´Ø¯ØŒ Ø¨Ø§ Ù…ØªÙ† Ø®Ø§Ù„ÛŒ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… ØªØ§ Ù¾Ø§Ø³Ø® Ø­Ø¯Ø§Ù‚Ù„ÛŒ Ø¨Ø±Ú¯Ø±Ø¯Ø¯
        context = ""

    # RAG: Ø³Ø§Ø®Øª Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ØŒ Ø¨Ø§ Ø¯Ø±ÙˆØ§Ø²Ù‡Ù” Ø§Ø³ØªÙ†Ø§Ø¯
    citations = []
    try:
        client_id_eff = request.cookies.get('client_id')
        if client_id_eff:
            _ensure_session_index(client_id_eff)
            retrieved = _retrieve_paragraphs(question, client_id_eff, top_k=5)
            # Ú¯ÛŒØª Ø³Ø§Ø¯Ù‡: Ø­Ø¯Ø§Ù‚Ù„ 2 Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø¨Ø§ Ø§Ù…ØªÛŒØ§Ø² >= 2 ÛŒØ§ Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§ >= 3
            total_score = sum(int(r.get('score') or 0) for r in retrieved)
            strong = [r for r in retrieved if (r.get('score') or 0) >= 2]
            if retrieved and (len(strong) >= 1 or total_score >= 3):
                citations = [{'score': int(r['score']), 'snippet': (r['snippet'][:400] + '...') if len(r['snippet']) > 400 else r['snippet']} for r in retrieved]
                rag_context = "\n\n---\n\n".join([c['snippet'] for c in citations])
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø²Ù…ÛŒÙ†Ù‡Ù” Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒâ€ŒØ´Ø¯Ù‡ Ø¨Ù‡â€ŒØ¬Ø§ÛŒ Ø²Ù…ÛŒÙ†Ù‡Ù” Ø®Ø§Ù…
                context = rag_context
    except Exception:
        pass

    answer = generate_answer(question, context)
    return jsonify({'answer': answer, 'citations': citations})

# ----------- Ø§Ø³ØªØ±ÛŒÙ… Ù¾Ø§Ø³Ø® (SSE) -----------
@app.route('/ask-stream', methods=['POST'])
def ask_question_stream():
    data = request.get_json()
    question = (data or {}).get('question')
    context = (data or {}).get('context') or ''
    if not question:
        def _bad():
            yield 'data: ' + '{"error":"Ø³Ø¤Ø§Ù„ Ø§Ø±Ø³Ø§Ù„ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª"}' + '\n\n'
        return Response(stream_with_context(_bad()), mimetype='text/event-stream')

    def _gen():
        import json as _json
        yield 'data: ' + _json.dumps({'type': 'status', 'message': 'retrieving'}) + '\n\n'
        citations = []
        try:
            cid = request.cookies.get('client_id')
            if cid:
                _ensure_session_index(cid)
                retrieved = _retrieve_paragraphs(question, cid, top_k=5)
                total_score = sum(int(r.get('score') or 0) for r in retrieved)
                strong = [r for r in retrieved if (r.get('score') or 0) >= 2]
                if retrieved and (len(strong) >= 1 or total_score >= 3):
                    citations = [{'score': int(r['score']), 'snippet': (r['snippet'][:400] + '...') if len(r['snippet']) > 400 else r['snippet']} for r in retrieved]
                    context_local = "\n\n---\n\n".join([c['snippet'] for c in citations])
                else:
                    context_local = context
            else:
                context_local = context
        except Exception:
            context_local = context
        yield 'data: ' + _json.dumps({'type': 'citations', 'items': citations}) + '\n\n'
        ans = generate_answer(question, context_local)
        yield 'data: ' + _json.dumps({'type': 'answer', 'text': ans}) + '\n\n'
    return Response(stream_with_context(_gen()), mimetype='text/event-stream')

# ----------- Ù¾Ù†Ù„ Ø§Ø¯Ù…ÛŒÙ† -----------
@app.route('/admin', methods=['GET'])
def admin_panel():
    return render_template('admin.html')

@app.route('/admin/stats', methods=['GET'])
def admin_stats():
    # Ø¢Ù…Ø§Ø± Ø³Ø§Ø¯Ù‡Ù” Ø³Ø´Ù†â€ŒÙ‡Ø§ Ùˆ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡
    with _context_lock:
        session_count = len(SESSION_CONTEXTS)
        total_texts = TOTAL_TEXTS_COUNT
        processed_files = PROCESSED_FILES_COUNT
    # ÙˆØ¶Ø¹ÛŒØª Ollama
    ollama_status = 'disabled'
    try:
        if _should_use_ollama():
            import json as _json
            import urllib.request as _u
            host = os.getenv('OLLAMA_HOST', 'http://127.0.0.1:11434')
            with _u.urlopen(host.rstrip('/') + '/api/tags', timeout=3) as r:
                _ = _json.loads(r.read().decode('utf-8'))
                ollama_status = 'online'
        else:
            ollama_status = 'disabled'
    except Exception:
        ollama_status = 'offline'

    # ÙˆØ¶Ø¹ÛŒØª LLaMA (Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„)
    llama_status = 'disabled'
    use_llama = _should_use_llama()
    llama_model = os.getenv('LLAMA_MODEL', '')
    if use_llama:
        if llama_model and os.path.exists(llama_model):
            llama_status = 'configured'
        else:
            llama_status = 'missing_model'

    # DeepSeek ÙˆØ¶Ø¹ÛŒØª
    deepseek_key = os.getenv('DEEPSEEK_API_KEY', '').strip()
    deepseek_status = 'configured' if deepseek_key else 'disabled'
    deepseek_model = os.getenv('DEEPSEEK_MODEL', 'deepseek-chat')

    # Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÙˆØªÙˆØ± ÙØ¹Ø§Ù„: DeepSeek > LLaMA > Ollama > local
    engine = 'local'
    if deepseek_status == 'configured':
        engine = 'deepseek'
    elif use_llama and llama_status == 'configured':
        engine = 'llama'
    elif _should_use_ollama() and ollama_status == 'online':
        engine = 'ollama'

    # ÙˆØ¶Ø¹ÛŒØª Ø¨Ø±Ø¯Ø§Ø±ÛŒ
    vector_ready = GLOBAL_VECTOR.get('index') is not None or (os.path.exists(os.path.join(VECTOR_INDEX_DIR, 'index.faiss')))

    data = {
        'sessions': session_count,
        'texts_in_memory': total_texts,
        'processed_files': processed_files,
        'default_pdf_dir': DEFAULT_PDF_DIR,
        'fast_mode': _is_fast_mode(),
        'ollama': ollama_status,
        'ollama_host': os.getenv('OLLAMA_HOST', 'http://127.0.0.1:11434'),
        'ollama_model': os.getenv('OLLAMA_MODEL', 'gemma3:4b'),
        'llama': llama_status,
        'llama_model': llama_model,
        'deepseek': deepseek_status,
        'deepseek_model': deepseek_model,
        'engine': engine,
        'model_status': MODEL_STATUS,
        'vector_ready': bool(vector_ready),
        'vector_model': GLOBAL_VECTOR.get('model_name') or os.getenv('EMBED_MODEL', ''),
    }
    return jsonify(data)

@app.route('/admin/config', methods=['GET', 'POST'])
def admin_config():
    if request.method == 'GET':
        cfg = _read_config_file()
        # Ù…Ø§Ø³Ú© Ú©Ø±Ø¯Ù† Ú©Ù„ÛŒØ¯Ù‡Ø§
        masked = dict(cfg)
        if masked.get('DEEPSEEK_API_KEY'):
            masked['DEEPSEEK_API_KEY'] = _mask_secret(str(masked['DEEPSEEK_API_KEY']))
        return jsonify(masked)
    # POST
    data = request.get_json(silent=True) or {}
    cfg = _read_config_file()
    # ÙÙ‚Ø· Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ù…Ø¬Ø§Ø²
    allowed = {
        'DEEPSEEK_API_KEY', 'DEEPSEEK_MODEL',
        'USE_OLLAMA', 'USE_LLAMA', 'USE_DEEPSEEK'
    }
    for k, v in (data.items() if isinstance(data, dict) else []):
        if k in allowed:
            cfg[k] = v
    ok = _write_config_file(cfg)
    if ok:
        _load_persisted_config_into_env()
        return jsonify({'ok': True})
    return jsonify({'ok': False}), 500

@app.route('/admin/clear', methods=['POST'])
def admin_clear():
    # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡Ù” Ù…ÙˆÙ‚Øª Ø³Ø´Ù†â€ŒÙ‡Ø§
    with _context_lock:
        SESSION_CONTEXTS.clear()
        SESSION_PROCESSED.clear()
    return jsonify({'message': 'Ø­Ø§ÙØ¸Ù‡Ù” Ø³Ø´Ù†â€ŒÙ‡Ø§ Ù¾Ø§Ú© Ø´Ø¯.'})

@app.route('/admin/reindex', methods=['POST'])
def admin_reindex():
    # Ø¨Ø§Ø²Ø®ÙˆØ§Ù†ÛŒ Ù¾ÙˆØ´Ù‡Ù” Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ùˆ Ø°Ø®ÛŒØ±Ù‡Ù” Ù…ØªÙ†â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ø³Ø´Ù† Ø¬Ø¯ÛŒØ¯ Ù…ÙˆÙ‚ØªÛŒ
    client_id = uuid.uuid4().hex
    saved_texts, previews, total_files = ingest_pdfs_from_dir(DEFAULT_PDF_DIR, recursive=True, client_id=client_id)
    if saved_texts:
        _append_session_context(client_id, saved_texts)
    return jsonify({
        'message': 'Ø¨Ø§Ø²Ø®ÙˆØ§Ù†ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.',
        'client_id': client_id,
        'total_files': total_files,
        'previews_count': len(previews)
    })

@app.route('/admin/build-vector', methods=['POST'])
def admin_build_vector():
    # Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø§Ø² Ù‚ÙˆØ§Ù†ÛŒÙ†/Ù…ØªÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯
    try:
        res = build_global_vector_index()
        return jsonify(res)
    except Exception as exc:
        return jsonify({'built': False, 'reason': str(exc)}), 500

@app.route('/admin/web-ingest', methods=['POST'])
def admin_web_ingest():
    """Ø¯Ø±ÛŒØ§ÙØª Ù…Ù†Ø§Ø¨Ø¹ Ø¹Ù…ÙˆÙ…ÛŒ Ø§Ø² ÙˆØ¨ (URLÙ‡Ø§)ØŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ù¾ÙˆØ´Ù‡Ù” sources."""
    data = request.get_json(silent=True) or {}
    urls = data.get('urls') or []
    add_to_vector = bool(data.get('add_to_vector', False))
    timeout_sec = float(data.get('timeout', 15))
    if isinstance(urls, str):
        urls = [u.strip() for u in urls.split('\n') if u.strip()]
    if not isinstance(urls, list) or not urls:
        return jsonify({'error': 'ÙÙ‡Ø±Ø³Øª URLs Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª'}), 400
    import urllib.request
    import hashlib
    saved_files = []
    texts = []
    for u in urls:
        try:
            req = urllib.request.Request(u, headers={'User-Agent': 'Mozilla/5.0'})
            with urllib.request.urlopen(req, timeout=timeout_sec) as resp:
                raw = resp.read()
                try:
                    html = raw.decode('utf-8', errors='ignore')
                except Exception:
                    html = raw.decode(errors='ignore')
                text = extract_text_from_html(html)
                if text:
                    h = hashlib.sha1(u.encode('utf-8', errors='ignore')).hexdigest()[:12]
                    outp = os.path.join(SOURCES_DIR, f'{h}.txt')
                    with open(outp, 'w', encoding='utf-8') as f:
                        f.write(text)
                    saved_files.append(outp)
                    texts.append(text)
        except Exception:
            continue
    # Ø¯Ø± ØµÙˆØ±Øª Ø¯Ø±Ø®ÙˆØ§Ø³ØªØŒ Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
    appended = 0
    if add_to_vector and texts and GLOBAL_VECTOR.get('index') is not None:
        model, _m = _get_embedder()
        if model is not None:
            import numpy as np  # type: ignore
            import faiss  # type: ignore
            paras_new = []
            for t in texts:
                paras_new.extend(_split_paragraphs(t))
            paras_new = [p for p in paras_new if p and len(p.strip()) >= 20]
            if paras_new:
                emb = model.encode(paras_new, convert_to_numpy=True, normalize_embeddings=True)
                try:
                    GLOBAL_VECTOR['index'].add(emb)
                    GLOBAL_VECTOR['paragraphs'].extend(paras_new)
                    appended = len(paras_new)
                except Exception:
                    appended = 0
    return jsonify({'saved': len(saved_files), 'files': saved_files, 'appended_to_vector': appended})

@app.route('/admin/curate', methods=['POST'])
def admin_curate_answer():
    """Ø«Ø¨Øª Ù¾Ø§Ø³Ø® Ø¨Ø§Ø²Ø¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡ ØªÙˆØ³Ø· Ø§Ù†Ø³Ø§Ù† Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ø³Ø¤Ø§Ù„ (HITL)."""
    data = request.get_json(silent=True) or {}
    question = (data.get('question') or '').strip()
    answer = (data.get('answer') or '').strip()
    if not question or not answer:
        return jsonify({'error': 'question/answer Ø§Ù„Ø²Ø§Ù…ÛŒ Ø§Ø³Øª'}), 400
    key = _hash_key_for_question(question)
    CURATED_ANSWERS[key] = { 'answer': answer, 'by': 'admin' }
    return jsonify({'ok': True, 'key': key})

@app.route('/admin/curate', methods=['GET'])
def admin_list_curated():
    out = []
    for k, v in CURATED_ANSWERS.items():
        out.append({ 'key': k, 'len': len(str(v.get('answer') or '')) })
    return jsonify({ 'count': len(out), 'items': out })

@app.route('/admin/set-deepseek', methods=['POST'])
def admin_set_deepseek():
    data = request.get_json(silent=True) or {}
    key = (data.get('api_key') or '').strip()
    model = (data.get('model') or '').strip() or 'deepseek-chat'
    if not key:
        return jsonify({'ok': False, 'error': 'api_key Ø§Ù„Ø²Ø§Ù…ÛŒ Ø§Ø³Øª'}), 400
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¯Ø± env
    cfg = _read_config_file()
    cfg['DEEPSEEK_API_KEY'] = key
    cfg['DEEPSEEK_MODEL'] = model
    cfg['USE_DEEPSEEK'] = '1'
    cfg['USE_OLLAMA'] = '0'
    cfg['USE_LLAMA'] = '0'
    if _write_config_file(cfg):
        _load_persisted_config_into_env()
        return jsonify({'ok': True, 'model': model})
    return jsonify({'ok': False}), 500

@app.route('/webhook', methods=['POST'])
@limiter.limit(os.getenv('RATE_LIMIT_WEBHOOK', '30 per minute'))
def webhook_receiver():
    # ØªÙˆÚ©Ù† ÙˆØ¨Ù‡ÙˆÚ© Ø§Ø² Ù‡Ø¯Ø± ÛŒØ§ querystring Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
    expected = os.getenv('WEBHOOK_TOKEN') or os.getenv('WEBHOOK_SECRET') or os.getenv('TOKEN')
    if expected:
        provided = (
            request.headers.get('X-Webhook-Token')
            or request.headers.get('X-Token')
            or request.args.get('token')
        )
        if not provided or provided != expected:
            return jsonify({'ok': False, 'error': 'unauthorized'}), 401

    # Ø§Ø®ØªÛŒØ§Ø±ÛŒ: Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø§Ù…Ø¶Ø§ (HMAC-SHA256) Ø±ÙˆÛŒ Ø¨Ø§Ø¯ÛŒ Ø®Ø§Ù…
    hsecret = os.getenv('WEBHOOK_HMAC_SECRET')
    if hsecret:
        try:
            raw = request.get_data(cache=True) or b''
            sig = hmac.new(hsecret.encode('utf-8'), raw, hashlib.sha256).hexdigest()
            provided_sig = request.headers.get('X-Signature') or request.headers.get('X-Hub-Signature-256')
            if not provided_sig or (sig.lower() != provided_sig.strip().lower().replace('sha256=','')):
                return jsonify({'ok': False, 'error': 'bad_signature'}), 401
        except Exception:
            return jsonify({'ok': False, 'error': 'sig_check_failed'}), 400

    payload = request.get_json(silent=True) or {}
    # Ø°Ø®ÛŒØ±Ù‡Ù” payload Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡Ù” Ø¢ØªÛŒ/Ø§Ø³Ú©Ø±ÛŒÙ¾Øª
    try:
        import json as _json
        import time as _time
        save_dir = os.path.join(BASE_DIR, 'tmp')
        os.makedirs(save_dir, exist_ok=True)
        fname = f"webhook_{int(_time.time())}.json"
        fpath = os.path.join(save_dir, fname)
        with open(fpath, 'w', encoding='utf-8') as f:
            _json.dump(payload, f, ensure_ascii=False)
    except Exception:
        fpath = ''

    # Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø®ØªÛŒØ§Ø±ÛŒ ÙØ§ÛŒÙ„ .bat Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ payload
    started = False
    bat_path = os.getenv('WEBHOOK_BAT')
    if bat_path and os.path.isfile(bat_path):
        try:
            import subprocess as _sub
            # Ø§Ø¬Ø±Ø§ÛŒ ØºÛŒØ±Ø¨Ù„Ø§Ú©â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¨Ø§ Ø§Ø±Ø³Ø§Ù„ Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ payload Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù† Ø§ÙˆÙ„
            _sub.Popen(['cmd.exe', '/C', bat_path, fpath or ''], cwd=BASE_DIR)
            started = True
        except Exception:
            started = False

    return jsonify({'ok': True, 'payload_file': fpath, 'bat_started': started})

if __name__ == '__main__':
    host = os.getenv('HOST', '127.0.0.1')
    try:
        port = int(os.getenv('PORT', '5051'))
    except ValueError:
        port = 5051
    # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ø¬Ø±Ø§ Ø±ÙˆÛŒ 127.0.0.1:5000 Ø¨Ù‡ ØµÙˆØ±Øª Ù‚Ø·Ø¹ÛŒ
    if host.strip() == '127.0.0.1' and port == 5000:
        logger.info("PORT=5000 Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³ØªØ› ØªØºÛŒÛŒØ± Ø¨Ù‡ 5051")
        port = 5051
    app.run(host=host, port=port, debug=False, use_reloader=False)
